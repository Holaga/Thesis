\chapter{Background}
\section{Machine Learning} \label{Machine Learning}
    As described in the book Deep Learning\cite{Goodfellow-et-al-2016_ML} machine learning can intuitively be split into four parts; The algorithm, empirical data, a task and a performance measure. A machine learning algorithm can then be identified as an algorithm that increases its performance on a task, given data. As this happens, the algorithm is said to be learning. The task itself and the data the algorithm is given may vary. This is why we can approximately divide the machine learning approaches into three categories\cite{Goodfellow-et-al-2016_E}: supervised learning, unsupervised and reinforcement learning. 
    
    \subsection{Algorithm types} \label{Algorithm types}
        \subsubsection{Supervised learning}
            Supervised learning \cite{Goodfellow-et-al-2016_E} algorithms base themselves on datasets containing samples that also have a label. This means the output the algorithm will have to predict. These labels can for example be binary class or consist of a multitude of classes or values in regression problems.
            
        \subsubsection{Unsupervised learning}
            Unlike supervised leaning, the unsupervised learning \cite{Goodfellow-et-al-2016_E} algorithms only have the data and will learn properties contained in the data. A practical example is clustering, where you can divide a dataset into clusters based on similar features. 
                
        \subsubsection{Reinforcement learning}
            In reinforcement learning \cite{Goodfellow-et-al-2016_E}, the algorithm do not learn from a given dataset, but will act in an environment. In some cases this is a feedback loop giving either a positive or negative reward for performing certain actions. The goal is then for the algorithm to maximize this reward. This is what people often associate with \gls{ai} and can be for example seen in the AlphaZero software that beat professional chess players\cite{silver2017mastering}.
    
    \subsection{Data}
        things
    \subsection{Features}
        things
    \subsection{Overfitting vs. underfitting}
        things
    \subsection{Bias - variance tradeoff}
        things
    \subsection{Model evaluation}
        things
    \subsubsection{Train-Val-Test split}
        things
    \subsubsection{F1-score, Precision, Recall} \label{f1_score}
        things
    \subsubsection{Hyperparameter-search}
        things

\section{Artificial Neural Networks} \label{neural networks}
    \citeauthor{Goodfellow-et-al-2016_NN} \cite{Goodfellow-et-al-2016_NN} describes \gls{ann} as a unknown function \textit{\^{f}} that maps an input \textit{x} to an output \textit{y}. The goal is then to approximate some optimal function \textit{f} through learning from examples. In this section, I will explain the basics of \gls{ann}s.

    \subsection{Perceptron} \label{perceptron}
        The \gls{ann}s fundamental building block is called an artificial neuron or perceptron. It is formulated in the following way\cite{razavi2021deep_exp_per}:
            \begin{equation} \label{eq_perceptron}
                y = \sigma(\sum_{i=1}^{D}w_ix_i + b)
            \end{equation}
            
        where D is the number dimension of the input space, x is the input vector and w is a set of weights that is of the same size as x, b is the bias and {\textsigma} is a nonlinear activation function which will be explained later in \ref{activation function}. In short the equation inside the activation functions is a linear regression. The perceptron is illustrated in figure \ref{Perceptron / MLP}.
    
    \subsection{Multi-layered perceptron} \label{MLP}
        The neurons presented in section \ref{perceptron} are then piled together in layers to form a \gls{ann}, which in turn forms what is called a \gls{mlp}.All the neurons in each layer are connected to every neuron in the next layer, as depicted in figure \ref{Perceptron / MLP}.
        
            \begin{figure}[H]
                \centering
                \includegraphics[scale=0.5]{figures/perceptron.png}
                \caption{(a) A perceptron and (b) a multi-layer perceptron with four inputs in the input-layer, two hidden layers(green), and three outputs in the output-layer(red).}
              	\medskip 
                \hspace*{15pt}\hbox{\scriptsize Credit: \citeauthor{razavi2021deep_exp_DL}\cite{razavi2021deep_exp_DL}}
                \label{Perceptron / MLP}
            \end{figure}
        
        The architecture of the \gls{ann} consist of an input layer, a user defined number of hidden layers and finally an output layer. A \gls{mlp} is a type of network called feed-forward \gls{ann} because the data flows from the input to the output layer. The parameters that are adjusted through training (explained in section XXX) are all the weights and biases between every neuron in the network. The intuition for the MLP depicted in \ref{Perceptron / MLP} is that different neurons will fire with varying strengths depending on the input, resulting in different outputs.
        
    \subsection{Activation function} \label{activation function}
        The activations function is what enables the \gls{ann} to learn non-linear features \cite{razavi2021deep_exp_per}. The reason you need it is that a network consisting of only linear layers will just be the same as a single linear layer \cite{razavi2021deep_exp_per}. The activation function used in this thesis was the ReLU which stands for rectified as described in \cite{sharma2019new_activation_func}. The formula for it is as follows:
            \begin{equation} \label{relu_eq}
                f(x) = max(0,x)
            \end{equation}
        As can be seen in figure \ref{activation_fig} the function is 0 while x is less than 0 and then linear. Intuitively, this function can make a selection of neurons in figure \ref{MLP} send their computed value forth, and some  other neurons output nothing. This can result in greater efficiency and faster training, as not all neurons are active \cite{sharma2019new_activation_func}.
        
        An example of another activation function is the sigmoid \cite{sharma2019new_activation_func}, which transforms the values in the range 0 to 1. I will list the formula and have included it in the plot \ref{activation_fig}, but will not go any further as it is not necessary for understanding this thesis. Sigmoid formula:
            \begin{equation} \label{sigmoid_eq}
                f(x) = \dfrac{1}{e^{-x}} 
            \end{equation}
            
            \begin{figure}[H]
                \centering
                \includegraphics[scale=0.5]{figures/activation.png}
                \caption{A ReLU function (blue) and a sigmoid function (red)}.
              	\medskip 
                \label{activation_fig}
            \end{figure}

\section{Training Neural Networks} \label{training neural networks}
    thing
\subsection{Backpropagation}
    things
\subsection{Gradient Decent}
    things
\subsection{Batch learning}
    things
\subsection{Regularization}
    things
\subsection{Batch-Norm}
    things
\subsection{Data-Augmentation}
    things

\section{Computer vision}
    things
\subsection{Convolutions ANNs}
    things
\subsection{Max-pool}
    things
\subsection{Skip connections}
    things
\subsection{Segmentation}
    things

\section{U-Net}
    In this part I introduce the architecture of the deep learning \gls{ann} used in this thesis called U-Net. This is a fully convolutional state-of-the-art\cite{rajak2021segmentation} semantic segmentation \gls{ann} developed for biomedical use by \citeauthor{unet_ronneberger2015}\cite{unet_ronneberger2015}. The \gls{crimac} used this model in their project (described in \ref{unet_paper_acoustic}) and had to do some small modifications to the network to fit their task. This modified U-Net is the one presented in this section.
    
    

\subsection{The architecture}
    The 
    
    
    
    \begin{figure}[H]
        \centering
        \includegraphics[scale=0.6]{figures/unet.png}
        \caption{asd}
      	\medskip 
        \label{unet_fig}
        \hspace*{15pt}\hbox{\scriptsize Credit: \citeauthor{brautaset2020acoustic}\cite{brautaset2020acoustic}}
    \end{figure}
    
    
\section{Accoustic data}
    things



\section{The data and tools}
    The data was provided by the \gls{imr} and I had access to a broad selection of yearly trawl cruises spanning from 2011 to 2020. In this chapter, I will delve into the data itself to give you a clearer picture of how it looked, how I acquired it and what tools I utilized.
    
    \subsection{The data provided (.RAW)}
        The files were initially presented to me as ".RAW" \cite{raw} and ".WORK" files. ".RAW" is the uncompressed raw output from the echo sounder. The same format is used by cameras before they are converted to, for example, ".JPEG". Because they are uncompressed, they do not lose any data. Unfortunately, this also makes them large. The ".WORK" files are the annotations of the ".RAW" files done by operators using a system called the \Gls{lsss}\cite{lsss}. The data from 2020 alone which spanned three months took 240Â GB of storage space and stored on a remote server.
        
        SJEKK DATAEKSEMPEL!
    \subsection{Windows Azure}
        Windows Azure \cite{azure} is a platform owned by Microsoft that provides cloud solutions for several services. My use was to access a remote storage provided by the \gls{imr} and mount this to my local computer. Thus enabling me to download the data for this thesis from \gls{imr}s server. 
    
    \subsection{Docker}
        Docker \cite{docker} is an open source platform that provides what they call containerization and is owned by the company under the same name, Docker, Inc. If you are familiar with virtual machines, then containerization will be very familiar to you. Docker is based on the Linux kernel, and enables you to create a container, which is an independent process that uses resources from the main instance. For each container, you can manage its own dependencies like programming languages and libraries. These containers can then be shared with others as images files, and as they can be run without the receiver having to manage the aforementioned dependencies as this is built into the image. Thus, you can make an application or code easily accessible for other people, as long as they have installed Docker.
        
    \subsection{Zarr}
        By using the ".ZARR" \cite{zarr} format, you gain access to store chunked compressed  multidimensional arrays. There are several highlights from this library, but I used primarily the ability to access the arrays on disk. This means I did not need to load the entire array into memory and could work with the array and access parts of it without hardware limitations.
        
    \subsection{CRIMAC-pipeline} \label{CRIMAC-pipeline}
        Together, the \gls{nr} and the \gls{imr} developed a pipeline\cite{crimac_pipeline} to classify the acoustic backscatter in echo sounder data. This was part of the work described earlier in \ref{unet_paper_acoustic} and is accessed by using docker. They also provide the image for a container to access the data through Azure.
        
        The pipeline is run using one docker container, which in turn downloads and runs four others:

            \begin{description}
              \item[$\bullet$ Preprocessor] Preprocesses the ".RAW" and ".WORK" to respectively ".ZARR" and ".PARQUET" files. Since frequencies can have different resolutions, all are re-gridded to the 38kHz frequency.
              \item[$\bullet$ Unet] Using a pretrained deep learning model called Unet it produces pixel based annotations
              \item[$\bullet$ Bottom detection] Identifies the bottom and generates a pixel based map stored as ".ZARR".
              \item[$\bullet$ Report generation] Takes in the output from the bottom detection, Unet and the preprocessed ".RAW" file and generates a report for the \gls{ices}.
    
            \end{description}

        The Unet, bottom detection and preprocessing is the same as described in section \ref{unet_paper_acoustic}.

    \subsection{Xarray}
        Xarray\cite{xarray} is a Python package that is made for working with multidimensional arrays. It is based on NumPy and adds labels in the form of attributes and coordinates on top of the NumPy-arrays. This was the library I used for accessing and working more efficiently with the ".ZARR" arrays, as I was already very familiar with NumPy.
        
        
    \subsection{Pytorch} \label{Pytorch}
    
    