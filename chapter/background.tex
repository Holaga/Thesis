\chapter{Background}

\section{The sandeel}
    

\section{The sandeel survey and acoustic data} \label{acoustic_data}
    things


\section{Machine Learning} \label{Machine Learning}
    As described in the book Deep Learning\cite{Goodfellow-et-al-2016_ML} machine learning can intuitively be split into four parts; The algorithm, empirical data, a task and a performance measure. A machine learning algorithm can then be identified as an algorithm that increases its performance on a task, given data. As this happens, the algorithm is said to be learning. The task itself and the data the algorithm is given may vary. This is why we can approximately divide the machine learning approaches into three categories\cite{Goodfellow-et-al-2016_E}: supervised learning, unsupervised and reinforcement learning. 
    
    \subsection{Algorithm types} \label{Algorithm types}
        \subsubsection{Supervised learning}
            Supervised learning \cite{Goodfellow-et-al-2016_E} algorithms base themselves on datasets containing samples that also have a label. This means the output the algorithm will have to predict. These labels can for example be binary class or consist of a multitude of classes or values in regression problems.
            
        \subsubsection{Unsupervised learning}
            Unlike supervised leaning, the unsupervised learning \cite{Goodfellow-et-al-2016_E} algorithms only have the data and will learn properties contained in the data. A practical example is clustering, where you can divide a dataset into clusters based on similar features. 
                
        \subsubsection{Reinforcement learning}
            In reinforcement learning \cite{Goodfellow-et-al-2016_E}, the algorithm do not learn from a given dataset, but will act in an environment. In some cases this is a feedback loop giving either a positive or negative reward for performing certain actions. The goal is then for the algorithm to maximize this reward. This is what people often associate with \gls{ai} and can be for example seen in the AlphaZero software that beat professional chess players\cite{silver2017mastering}.
    
    \subsection{Data}
        things
    \subsection{Features}
        things
    \subsection{Overfitting vs. underfitting}
        things
    \subsection{Bias - variance tradeoff}
        things
    \subsection{Model evaluation}
        things
    \subsubsection{Train-Val-Test split}
        things
    \subsubsection{F1-score, Precision, Recall} \label{f1_score}
        things
    \subsubsection{Hyperparameter-search}
        things

\section{Artificial Neural Networks} \label{neural networks}
    In this section, I will introduce the basic components of an \gls{ann} and how these are combined to create a complete network.

    \subsection{Perceptron} \label{perceptron}
        The \gls{ann}s fundamental building block is called an artificial \textit{neuron} or perceptron. It is formulated in the following way\cite{razavi2021deep_exp_per}:
            \begin{equation} \label{eq_perceptron}
                y = \sigma(\sum_{i=1}^{D}w_ix_i + b)
            \end{equation}
            
        where D is the number dimension of the input space, x is the input vector and w is a set of weights that is of the same size as x, b is the bias and {\textsigma} is a nonlinear activation function which will be explained later in \ref{activation function}. In short, the equation inside the activation functions is a linear regression with the tunable parameters w and b. The perceptron is illustrated in figure \ref{Perceptron / MLP}.
    
    \subsection{Multi-layered perceptron} \label{MLP}
        The neurons presented in section \ref{perceptron} are then copied and sorted together in layers to form a \gls{ann}, which in turn forms what is called a \gls{mlp} \cite{razavi2021deep_exp_per}. All the neurons in each layer are connected to every neuron in the next layer, as depicted in figure \ref{Perceptron / MLP}.
        
            \begin{figure}[H]
                \centering
                \includegraphics[scale=0.5]{figures/perceptron.png}
                \caption[The perceptron and multi-layer perceptron]{(a) A perceptron and (b) a multi-layer perceptron with four inputs in the input-layer(arrows to the left), two hidden layers(green), and three outputs in the output-layer(red).}
              	\medskip 
                \hspace*{15pt}\hbox{\scriptsize Credit: \citeauthor{razavi2021deep_exp_DL}\cite{razavi2021deep_exp_DL}}
                \label{Perceptron / MLP}
            \end{figure}
        
        The architecture of the \gls{ann} consist of an input layer, a user defined number of \textit{hidden layers} and finally an output layer. A \gls{mlp} is a type of network called \textit{feed-forward} \gls{ann} because the data flows from the input to the output layer. The parameters that are adjusted through training (explained in section XXX) are all the weights between every neuron in the network and the individual biases. The intuition for the MLP depicted in \ref{Perceptron / MLP} is that different neurons will fire with varying strengths depending on the input, resulting in different outputs.
        
    \subsection{Activation function} \label{activation function}
        The activations function is what enables the \gls{ann} to learn non-linear features \cite{razavi2021deep_exp_per}. The reason you need it is that a network consisting of only linear layers will just be the same as a single linear layer \cite{razavi2021deep_exp_per}. The activation function used in this thesis was the ReLU which stands for rectified as described in \cite{sharma2019new_activation_func}. The formula for it is as follows:
            \begin{equation} \label{relu_eq}
                f(x) = max(0,x)
            \end{equation}
        As can be seen in figure \ref{activation_fig} the function is 0 while x is less than 0 and then linear. Intuitively, this function can make a selection of neurons in figure \ref{MLP} send their computed value forth, and some  other neurons output nothing. This can result in greater efficiency and faster training, as not all neurons are active \cite{sharma2019new_activation_func}.
        
        An example of another activation function is the sigmoid \cite{sharma2019new_activation_func}, which transforms the values in the range 0 to 1. I will list the formula and have included it in the plot \ref{activation_fig}, but will not go any further as it is not necessary for understanding this thesis. Sigmoid formula:
            \begin{equation} \label{sigmoid_eq}
                f(x) = \dfrac{1}{e^{-x}} 
            \end{equation}
            
            \begin{figure}[H]
                \centering
                \includegraphics[scale=0.5]{figures/activation.png}
                \caption[ReLu and sigmoid]{A ReLU function (blue) and a sigmoid function (red)}.
              	\medskip 
                \label{activation_fig}
            \end{figure}

\section{Training Neural Networks} \label{training neural networks}
    \citeauthor{Goodfellow-et-al-2016_NN}\cite{Goodfellow-et-al-2016_NN} describes \gls{ann} as an unknown function \textit{\^{f}} that maps an input \textit{x} to an output \textit{y}. The goal is then to approximate some optimal function \textit{f} through learning from examples. In supervised tasks, the data instructs the output layer exactly how it should perform and what \textbf{y} should look like. Although, the data does not instruct the individual \textit{hidden layers} how to perform to produce this desired output. When the data flow through the network, it produces outputs \textbf{\^{y}} and is called the \textit{forward-propagation step}. By using a \textit{loss function} comparing the true \textbf{y} values to the estimated values \textbf{\^{y}}, you get a measurement of the network's error. Using this error, you step back through the network in a process called \textit{back-propagation} and calculate a value called the \textit{gradient}. The gradient indicates how each weight and bias in the hidden layers should be adjusted to decrease the networks' error. Using the gradient, an optimizing algorithm (example in section \ref{batch learning}) is applied to adjust all the individual parameters, performing \textit{gradient descent}\cite{Goodfellow-et-al-2016_gradient_descent}. In what magnitude a parameter is adjusted by the optimizing algorithm is tuned through a value called the learning rate, usually less than 1, which decreases the chances of numerical overflow\cite{Goodfellow-et-al-2016_learning_rate} and promotes generalization\cite{wilson2001need_learning_rate}.
    
    In classification tasks, the network is trained to output the probability of a class given an input. To do this, \citeauthor{zhou2019mpce_cross_entropy}\cite{zhou2019mpce_cross_entropy} describes that a \textit{soft max} function that calculates the probability of each class with a sum of 1 across all classes as the last layer of a classification \gls{ann}. This probability is then the input to a loss function called \textit{cross entropy}, which calculate a loss based on probabilities. A variant of this function is called weighted cross entropy\cite{ho2019real_weighted_cross_entropy}, and with this you can adjust the weighting different classes have on the calculated loss. It is often used when dealing with class imbalances, and hence in this thesis.
    
    %By repeatedly applying forward-propagation, back-propagation and some optimizing algorithm on new examples as stated by \citeauthor{Goodfellow-et-al-2016_NN}, you train a network to approximate the optimal function \textit{f}.
    

\subsection{Batch learning} \label{batch learning}
        \begin{quote}
        "\textit{A recurring problem in machine learning is that large training sets are necessary for good generalization, but large training sets are also more computationally
        expensive."} - (\citeauthor{Goodfellow-et-al-2016_SGD}\citeyear{Goodfellow-et-al-2016_SGD}, page 152)
    \end{quote}
    
    The problem, described in the quote above, arises when you have a large dataset, and you would calculate the loss values of all samples before updating the parameters in your network\cite{Goodfellow-et-al-2016_SGD}. Depending on your hardware, this could lead to a crash or slow learning due to heavy memory demands. A solution is then to choose a number of uniformly chosen examples from the entire dataset forming a \textit{minibatch}, with the intent to approximately estimate the true gradient using this smaller fraction of the dataset. Then you would update the parameters of your network based on this and repeat on a new batch. When you have run this process on all your data, you say that an \textit{epoch} has passed. The size of this minibatch can vary from one example, to hundreds and the size chosen can heavily affect training and is very reliant on a fitting learning rate\cite{wilson2001need_learning_rate}. A widely used optimization algorithm is \textit{\gls{sgd}}\cite{Goodfellow-et-al-2016_SGD}, which utilizes this minibatch form of training. \gls{sgd} is the algorithm used is this thesis.
    
\subsection{Regularization}
    Regularization as described by \citeauthor{kukavcka2017_regularization}\cite{kukavcka2017_regularization} is a small alternation or technique added to the training process that has as goal to make the model generalize better on the validation and test data. This can be applied in different ways, and two used in this thesis will be described here; \textit{batch normalization} and \textit{data-augmentation}.
    
\subsubsection{Batch-Norm}

\subsubsection{Data-Augmentation}
    things

\clearpage
\section{Computer vision}
    \textbf{Computer vision} is a very popular field of research within deep learning\cite{voulodimos2018deep_computer_vision}. This is because it has mainly focused around tasks we humans do naturally, while computers would struggle. This could be facial recognition, object detection and many more. Some work also went beyond human capability, like \citeauthor{davis2014visual_deep_video_audio}s\cite{davis2014visual_deep_video_audio} work, where he recovered sounds from the vibrations they induced in objects captured on video. There are many methods in the field of computer vision, but I will focus on the \gls{cnn}.
    
    
    
\subsection{Convolutional neural network} \label{cnn}
    The \textbf{\gls{cnn}} are a type of \gls{ann}s that are primarily used in machine learning tasks concerning images\cite{o2015introduction_convolutions}.The need for the convolutional operation stemmed from the fact that images input to a regular \gls{ann} produces a large amount of learnable parameters. An example 512×512 low-resolution image with a first layer of 1 neuron would have $1*512*512 = 262144$ weights alone.  To solve this issue and have fewer learnable parameters, the \gls{cnn} is built around 3 main components\cite{o2015introduction_convolutions}; \textit{convolutional operation}, \textit{pooling operation} and a \textit{fully-connected layer}. Each of these will now be explained.

    \begin{figure}[H]
        \centering
        \includegraphics[scale=0.4]{figures/convolution.png}
        \caption[Convolutional operation example]{The example input is an image with the color channels red, green, and blue. The kernel is of height 3 and width 3, with a depth of 3 to correspond with the input number of channels. The yellow outlined area in the input illustrates the receptive field of the kernel, which produces an example output of 42. More kernels could be applied to increase the dimension of the output(orange). With only the yellow filter, this model would have; $3*3*3$ weights $+ 1$ bias $= 28$ learnable parameters, independent of input image height and width.}
      	\medskip 
        \label{convolutional_high_level_fig}
    \end{figure}
    
     
    
    
     \citeauthor{o2015introduction_convolutions}\cite{o2015introduction_convolutions} describes that the convolutional operation in a \gls{cnn} consists of a weight matrix of three dimensions; \textit{height}, \textit{width} and \textit{depth} written as: height x width x depth. From now, this matrix will be called the kernel. The height and the width of the kernel are parameters defined by the user, but the depth will always equal the number of channels in the input. This results in kernels being written only as: height x width.  The kernel is iteratively applied to different locations of the input, called the current \textit{receptive field}. When applied, it calculates the scalar product between the current receptive field and the kernel, producing a single scalar value called the \textit{activation}. The next receptive field is then located by moving the receptive field by a value defined as the \textit{stride} in the horizontal direction. If this is not possible due to a part of the  receptive field being outside the input, the receptive field will move rows down vertically equal to the stride and begin anew. The high level convolutional operation is illustrated in figure \ref{convolutional_high_level_fig}.
    
    \begin{figure}[H]
        \centering
        \includegraphics[scale=0.4]{figures/convolutions.png}
        \caption[Horizontal edge detector example]{Illustration of a \textit{valid} convolutional operation. The kernel is applied repeatedly across the input. The size of input is 6x6, kernel size is 3x3 and stride = 1, resulting in overlapping operations and output size being 4x4. The figures to the right show the input, kernel and output(activation) as color gradings, where the color gets darker if the values are low. This example is a horizontal edge detector, and the result is large activations in the output along the border between the values of 0 and 10 in the input, which could have been colors in a picture.}
      	\medskip 
        \label{convolutional_fig}
    \end{figure}

    The complete \textit{convolutional operation} performed by a convolutional layer consists of applying this kernel to the entire input. Producing activations for different regions of the input. This reduces the dimension of the input to a 2D activation output, but you may apply several kernels to increase the channels, as each will create a new 2D channel in the activations. A convolutional operation is often followed up with an element wise nonlinear activation function to its activations, like a ReLu. The kernels will through training learn to detect different features in the input, and so in a \gls{cnn} the first layers will often detect simple features like edges. Later layers will then detect more complex features like cars and houses, combining the activations from different earlier layers. The last layer consists of a \textit{fully connected} layer of regular neurons to determine the output, but implementations of networks with only convolutional operations exists like U-Net which will be described\cite{unet_ronneberger2015} in section \ref{unet} utilizing 1x1 convolutions. A more detailed example can be found in figure \ref{convolutional_fig} which looks at how a convolution with a horizontal edge detecting kernel is applied to a single channel or an equivalent 2D input. 
    
    
    Reductions in the spatial size\cite{o2015introduction_convolutions} will normally occur with the standard convolutions, and this type is called \textit{valid convolutions}. By applying a padding with zeros around the input, you can retain the dimensions of the input. The effect is that more convolutional operations fits in the new padded input, hence larger output size. This is called a \textit{same convolution}. Illustrated in figure \ref{same_convolutional_fig}.
    
    \begin{figure}[H]
        \centering
        \includegraphics[scale=0.5]{figures/same_convolutions.png}
        \caption[Same convolution example]{Illustration of a \textit{same} convolutional operation. The size of the input is 3×3, but after padding with zeros the size is 5×5, kernel size is 3×3 and stride is 1. Resulting in the size of the output being 3×3, hence conserving the input size.}
      	\medskip 
        \label{same_convolutional_fig}
    \end{figure}
    
    
    
\subsection{Max pool}
    As mentioned in the start of this chapter, the \textbf{max pool} operation reduces the size of its input\cite{o2015introduction_convolutions}. Like a convolutional operation, the max pool looks at a region of the input, but then instead applies a \textit{max} operation. The kernel size is given in \textit{height} x \textit{width}, and is applied individually to each dimension of the input.This reduces the height and width, but not the channels. The most common setting is 2x2 with a stride = 2, which results in decreasing the size to 25\%.

    \begin{figure}[H]
        \centering
        \includegraphics[scale=0.5]{figures/max_pool.png}
        \caption[The max pool operation]{Illustration of the max pool operation with size = 2x2 and stride = 2.}
      	\medskip 
        \label{maxpool_fig}
    \end{figure}
    
\subsection{Transposed convolutions}
    A transposed convolution is an operation performing the opposite operation compared to a regular convolution\cite{dumoulin2016guide_transposed_convolution}. This means taking an input and with a similar \textit{kernel} as described earlier in \ref{cnn} and now instead map this to a higher spatial size. In example figure \ref{transposed_conv_fig} a 2D 2×2 input is fed to a transposed convolutional layer with kernel size 2×2. The whole kernel is multiplied element wise to the input and proceeds to produce values in a temporary matrix initialized with zeros(empty cells in the figure). The calculated values in the temporary matrix are situated correctly relative to the input. These temporary matrices are then summed over, producing the output. This operation is repeated for all channels, retaining the channels in the input.
    
    \begin{figure}[H]
        \centering
        \includegraphics[scale=0.5]{figures/transpose_convolution.png}
        \caption[Transposed convolution]{Illustration of the transposed convolution operation with kernel size 2×2 and stride = 1. The green color shows one of the intermediate computations. The center value of each crop is outlined to illustrate the summation step as these overlaps.}
      	\medskip 
        \label{transposed_conv_fig}
    \end{figure}
    
\subsection{Advantages of the convolutional neural network}
    To sum up the advantages of the \gls{cnn}\cite{o2015introduction_convolutions}, they have, compared to regular \gls{ann}s, very few learnable parameters. It is only the kernels weights that are the parameters learned, and so share their parameters across all regions of the input, thus greatly reducing the amount. The max pool operation reduces the dimensions, hence reducing the size of the following convolutional layers, and does not have any learnable parameters itself. Since the same kernel is applied across the input, the position of what the kernel detects is also not relevant.
    
    
\subsection{Segmentation}
    Segmentation is a computer vision task where the objective is to assign one or several classification masks to the input\cite{He_2017_ICCV_segmentation}. This is again split into two different categories; \textit{semantic} and \textit{instance} segmentation. In semantic segmentation, you assign each pixel in the input to predefined classes. While in instance segmentation, you increase the complexity by applying semantic segmentation and in parallel assigning a bounding box to each individual object. Semantic segmentation is the variant used in this thesis, and examples of both can be viewed in figure \ref{segmentation_fig}.
    
    \begin{figure}[H]
        \centering
        \includegraphics[scale=0.4]{figures/segmentation.png}
        \caption[Difference between semantic and instance segmentation]{Illustration of the difference between semantic and instance segmentation.}
      	\medskip 
        \label{segmentation_fig}
    \end{figure}
    
    

\subsection{U-Net} \label{unet}
    In this part I introduce the architecture of the deep learning \gls{ann} used in this thesis called U-Net. This is a fully convolutional state-of-the-art\cite{rajak2021segmentation} semantic segmentation \gls{ann} and was initially developed for biomedical use by \citeauthor{unet_ronneberger2015}\cite{unet_ronneberger2015}. The \gls{crimac} used this model in their project (described in \ref{unet_paper_acoustic}) and had to do some small modifications to the network to fit their task. This modified U-Net is the one presented in this section, as this is the one used in the experiments. The core functionality of the network stays true to the original Unet and the alterations done to the original will be explained later in this section.
    
    \begin{figure}[H]
        \centering
        \includegraphics[scale=0.5]{figures/unet_arrows.png}
        \caption[U-Net architecture]{U-Net architecture, the downwards facing arrow illustrates the contracting path and the one facing upwards is the expanding path.}
      	\medskip 
        \label{unet_fig}
        \hspace*{15pt}\hbox{\scriptsize Credit: \citeauthor{brautaset2020acoustic}\cite{brautaset2020acoustic}}
    \end{figure}
    
    Unet utilized what \citeauthor{unet_ronneberger2015}\cite{unet_ronneberger2015} called a contracting path to identify what was in a picture, while an expanding path to localize where it was. These two branches were symmetrical, and together they formed a U-shape, giving the network its name. The contracting path can be looked at as five different stages of processing, from top to bottom, in figure \ref{unet_fig}. Each stage applied the same operations to its given input. For each stage, this consisted first of two 3x3 same convolutions with their individual ReLu activation functions. Initially, the feature channels would be increased to 64, and later doubled for each contracting stage. The convolutions were followed by a 2x2 max pooling operation with stride 2 to decrease the resolution of the output from the convolutional operations, and then send this feature map down to the next stage. At the bottom stage, the only things that changed was the use of transpose convolutions instead of max pooling to now increase the resolution. At each subsequent stage going back up the expanding path, the number of feature channels were halved. To aid the localization, the output of the previous stage would be concatenated with the output feature map of a stage from the contracting path with the same size. At the last stage, when the resolution had reached it original size, a 1x1 convolution was applied instead of increasing the resolution. The 1x1 convolution mapped the 64 feature channels to the 3 classes. The softmax was then calculated between these classes, giving each pixel a value between [0,1], summarized over all classes to 1. Hence, giving you a segmentation map of each class with the same size as the input. 
    
    %In this thesis, this will be a semantic segmentation of the classes \textit{background}, \textit{other } and most importantly \textit{sandeel}.
    
    Minor changes were made to adapt\citeauthor{unet_ronneberger2015}s original U'net to the acoustic data used in \ref{unet_paper_acoustic}, as the input now had the form 4 x 256 x 256. The four channels being the frequencies used. The convolutions were as mentioned in the previous paragraph set to \textit{same} instead of \textit{valid}, as were the original setting. This was done to make the size of the input and output match. Further, batch normalization was added to each convolution. 
    

% Unsure if this is to be here, in methodology or appendix.
\clearpage
\section{The data and tools}
    The data was provided by the \gls{imr} and I had access to a broad selection of yearly trawl cruises spanning from 2011 to 2020. In this chapter, I will delve into the data itself to give you a clearer picture of how it looked, how I acquired it and what tools I utilized.
    
    \subsection{The data provided (.RAW)}
        The files were initially presented to me as ".RAW" \cite{raw} and ".WORK" files. ".RAW" is the uncompressed raw output from the echo sounder. The same format is used by cameras before they are converted to, for example, ".JPEG". Because they are uncompressed, they do not lose any data. Unfortunately, this also makes them large. The ".WORK" files are the annotations of the ".RAW" files done by operators using a system called the \Gls{lsss}\cite{lsss}. The data from 2020 alone which spanned three months took 240 GB of storage space and stored on a remote server.
        
        SJEKK DATAEKSEMPEL!
    \subsection{Windows Azure}
        Windows Azure \cite{azure} is a platform owned by Microsoft that provides cloud solutions for several services. My use was to access a remote storage provided by the \gls{imr} and mount this to my local computer. Thus enabling me to download the data for this thesis from \gls{imr}s server. 
    
    \subsection{Docker}
        Docker \cite{docker} is an open source platform that provides what they call containerization and is owned by the company under the same name, Docker, Inc. If you are familiar with virtual machines, then containerization will be very familiar to you. Docker is based on the Linux kernel, and enables you to create a container, which is an independent process that uses resources from the main instance. For each container, you can manage its own dependencies like programming languages and libraries. These containers can then be shared with others as images files, and as they can be run without the receiver having to manage the aforementioned dependencies as this is built into the image. Thus, you can make an application or code easily accessible for other people, as long as they have installed Docker.
        
    \subsection{Zarr}
        By using the ".ZARR" \cite{zarr} format, you gain access to store chunked compressed  multidimensional arrays. There are several highlights from this library, but I used primarily the ability to access the arrays on disk. This means I did not need to load the entire array into memory and could work with the array and access parts of it without hardware limitations.
        
    \subsection{CRIMAC-pipeline} \label{CRIMAC-pipeline}
        Together, the \gls{nr} and the \gls{imr} developed a pipeline\cite{crimac_pipeline} to classify the acoustic backscatter in echo sounder data. This was part of the work described earlier in \ref{unet_paper_acoustic} and is accessed by using docker. They also provide the image for a container to access the data through Azure.
        
        The pipeline is run using one docker container, which in turn downloads and runs four others:

            \begin{description}
              \item[$\bullet$ Preprocessor] Preprocesses the ".RAW" and ".WORK" to respectively ".ZARR" and ".PARQUET" files. Since frequencies can have different resolutions, all are re-gridded to the 38kHz frequency.
              \item[$\bullet$ Unet] Using a pretrained deep learning model called Unet it produces pixel based annotations
              \item[$\bullet$ Bottom detection] Identifies the bottom and generates a pixel based map stored as “.ZARR”.
              \item[$\bullet$ Report generation] Takes in the output from the bottom detection, Unet and the preprocessed ".RAW" file and generates a report for the \gls{ices}.
    
            \end{description}

        The Unet, bottom detection and preprocessing is the same as described in section \ref{unet_paper_acoustic}.

    \subsection{Xarray}
        Xarray\cite{xarray} is a Python package that is made for working with multidimensional arrays. It is based on NumPy and adds labels in the form of attributes and coordinates on top of the NumPy-arrays. This was the library I used for accessing and working more efficiently with the ".ZARR" arrays, as I was already very familiar with NumPy.
        
        
    \subsection{Pytorch} \label{Pytorch}
    
    