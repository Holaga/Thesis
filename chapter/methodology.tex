\chapter{Methodology}
\section{Challenges and restrictions}
    \subsection{The ZARR issue}
        things
    \subsection{Bitmap errors} \label{bitmap_error}
        things
        
    \subsection{Server at the University of Bergen}
        things
        
    

\section{My solution}
    \subsection{Pseudo labels} \label{Pseudo label}
        Due to the bitmap errors mentioned in \ref{bitmap_error} I did not have access to the same labels that were used in the original work \ref{unet_paper_acoustic} and so had to find an alternate solution. This came to be based on what is called pseudo labels. In meetings with the \gls{imr} we came up with the idea to use the output of the model used in \ref{unet_paper_acoustic} as my labels. This was based on the fact that the models' performance were assessed to be sufficient for my task. I was then able to create as much data with corresponding pseudo labels as I deemed necessary.
        
        This would mean that the labels would consist of labels produced from the four frequencies used in article \ref{unet_paper_acoustic}, namely; 18kHz,38kHz,120kHz and 200kHz. My data contained the additional 70kHz and 333kHz frequencies and this may lead to a bad results for these two respectively as, the pseudo labels may be biased towards the original four frequencies. This is an observation I will return to in the discussion. 
        
        REF TIL DER DET TAS OPP
        
    \subsection{The final dataset}
        With the pseudo labels from section \ref{Pseudo label} I possessed all the components I needed to create my own dataset. Here I will go through my implementation of preprocessing the ".RAW" into a dataset able to be quickly loaded into a deep learning algorithm.
        \clearpage
        \begin{figure}[H]
            \centering
            \includegraphics[scale=0.8]{figures/flow_data_gen.png}
            \caption{An overview of the data generation process. CRIMAC pipeline modules (black), data.raw (blue), SV data.zarr (green), pseudo labels.zarr (yellow), bottom.zarr (red) and the finished file.pytorch (gray)}
          	\medskip 
            \label{data_generation_flowchart_fig}
        \end{figure}
        The entire process is illustrated in figure \ref{data_generation_flowchart_fig} and starts with the utilization of \gls{crimac} preprocessor module as described in \ref{CRIMAC-pipeline}. This takes in the .RAW data and outputs the \gls{sv} data in the ".ZARR" format. From the \gls{sv} data, a crop is then extracted and was checked for any missing values or being located entirely below the seafloor. This was repeated until the algorithm found a crop that was that did not trigger any of the previous two statements.
        
        In figure \ref{data_generation_flowchart_fig} the  \gls{sv} data were also sent to both the pretrained Unet and bottom detection modules from the \gls{crimac} pipeline. The Unet outputs the pseudo labels and the bottom detection outputs a mask of the seafloor. Then, using the vertical and horizontal coordinates from the \gls{sv} data, a corresponding crop from both the seafloor mask and the pseudo labels were extracted. The two new crops are used together to remove predictions appearing under the seafloor. The steps explained in this paragraph are also visualized in figure \ref{crop_extract_fig}.
        \clearpage
        \begin{figure}[H]]
            \centering
            \includegraphics[scale=0.5]{figures/crop_extract_illustration.png}
            \caption{Example of how the data, pseudo labels and bottom crops would look during data generation. For the pseudo label, purple is values of 0 and yellow value of 1. Some predictions under the seafloor are removed.  Size is shown to make it clear that it is a crop of the same size from the same location.}.
          	\medskip 
            \label{crop_extract_fig}
        \end{figure}
        
        After the crop containing the data and cleaned pseudo labels were generated, I combined them into one file that could be stored to the file system. The file was stored using pytorch (\ref{Pytorch}) based on some criterions that will be explained shortly, creating a data hierarchy as shown in figure \ref{data_hierarchy_fig}:
        
        
        \clearpage
        \begin{figure}[H]
            \centering
            \includegraphics[scale=0.5]{figures/data_hierarki.png}
            \caption{An overview of the data hierarchy.}.
          	\medskip 
            \label{data_hierarchy_fig}
        \end{figure}
        
        The criterions were based on those stated in the \ref{unet_paper_acoustic} article and can be seen in figure \ref{data_generation_flowchart_fig}. In the previously mentioned article, they had the real annotations to know if there was "sandeel" or instances of the "other" class in a crop. I had to rely on the pseudo labels for the same task, and as these were noisy, I had to counteract this. Hence, I set a filter where 100 of the pixels in the crop had to belong to a single class to satisfy my criteria. If there were not enough pixels of either class, the crop would be set to be "valid" with or without bottom. To mitigate duplicate samples, the criteria would be checked from the top to the bottom and of the criterion list and store the crop to disk depending on the first criterion hit.
        
        The process of generating data from a single year took around 1 to 3 days. This made me decide to not use all the data, but two years was sufficient for my initial experiment. In dialogue with the \gls{imr} I chose the year 2018 and 2019 as these were recommended. This was because they had few errors associated with them. The final data distribution can be seen in table \ref{data_distribution_table}.
        
        \clearpage
        \begin{table}[H]
        \caption{Data distribution.}.
        \begin{tabular}{|llll|l|l|lllll|l|l}
        \cline{1-5} \cline{7-12}
        \multicolumn{4}{|l|}{\textbf{2018 dataset}}                                             & \% of dataset &  & \multicolumn{5}{l|}{\textbf{2019 dataset}}                                             & \% of dataset &  \\ \cline{1-5} \cline{7-12}
        \multicolumn{2}{|l|}{\multirow{3}{*}{No bottom}} & \multicolumn{1}{l|}{Sandeel} & 1410  & 7.3\%         &  & \multicolumn{3}{l|}{\multirow{3}{*}{No bottom}} & \multicolumn{1}{l|}{Sandeel} & 1308  & 4.38\%        &  \\ \cline{3-5} \cline{10-12}
        \multicolumn{2}{|l|}{}                           & \multicolumn{1}{l|}{Other}   & 184   & 0.95\%        &  & \multicolumn{3}{l|}{}                           & \multicolumn{1}{l|}{Other}   & 1074  & 3.60\%         &  \\ \cline{3-5} \cline{10-12}
        \multicolumn{2}{|l|}{}                           & \multicolumn{1}{l|}{Valid}   & 6519  & 33.75\%       &  & \multicolumn{3}{l|}{}                           & \multicolumn{1}{l|}{Valid}   & 10291 & 34.45\%       &  \\ \cline{1-5} \cline{7-12}
        \multicolumn{2}{|l|}{\multirow{3}{*}{Bottom}}    & \multicolumn{1}{l|}{Sandeel} & 1297  & 6.71\%        &  & \multicolumn{3}{l|}{\multirow{3}{*}{Bottom}}    & \multicolumn{1}{l|}{Sandeel} & 1652  & 5.53\%        &  \\ \cline{3-5} \cline{10-12}
        \multicolumn{2}{|l|}{}                           & \multicolumn{1}{l|}{Other}   & 840   & 4.35\%        &  & \multicolumn{3}{l|}{}                           & \multicolumn{1}{l|}{Other}   & 2815  & 9.42\%        &  \\ \cline{3-5} \cline{10-12}
        \multicolumn{2}{|l|}{}                           & \multicolumn{1}{l|}{Valid}   & 9067  & 46.94\%       &  & \multicolumn{3}{l|}{}                           & \multicolumn{1}{l|}{Valid}   & 12733 & 42.62\%       &  \\ \cline{1-5} \cline{7-12}
        \multicolumn{3}{|l|}{\textbf{Total}}                                            & 19317 & 100\%         &  & \multicolumn{4}{l|}{\textbf{Total}}                                            & 29873 & 100\%         &  \\ \cline{1-5} \cline{7-12}
        \end{tabular}
        \label{data_distribution_table}
        \end{table}
        
        From the table \ref{data_distribution_table} I observed that the as in article \ref{unet_paper_acoustic} most of the data will contain no data, here represented as "valid". 
        
\section{Experiments}
    The experiments revolved around manipulating the amount of frequencies given as input to the model. To be able to judge the future model's performance, I had to establish a baseline. Hence, I trained a model on all the 6 available frequencies to establish a baseline model. This was done by using the 
    
    \subsection{Experiment settings}
        SKRIV OM HVORFOR DU IKKE BRUKER 5 PROSENT STÃ˜Y I PIKSLER I DISKUSJON
        
        All models are based on the one created in article \ref{unet_paper_acoustic}. This means the architecture, hyperparameters and the way the data is loaded into the model. A summary can be seen in table \ref{experiment_settings_table}.
        
        \clearpage
        \begin{table}[H]
        \caption{Overview of experiment parameters.}
        \label{experiment_settings_table}
        \begin{tabular}{lll}
        \cline{1-2}
        \multicolumn{1}{|l|}{\textbf{Data augmentation}}        & \multicolumn{1}{l|}{\textbf{Comments}}                     & \textbf{}                                                                                               \\ \cline{1-2}
        \multicolumn{1}{|l|}{\textit{Convert to decibel scale}} & \multicolumn{1}{l|}{Always}                                &                                                                                                         \\ \cline{1-2}
        \multicolumn{1}{|l|}{\textit{Flip along vertical axis}} & \multicolumn{1}{l|}{50\% of happening upon loading sample} &                                                                                                         \\ \cline{1-2}
                                                                &                                                            &                                                                                                         \\ \hline
        \multicolumn{1}{|l|}{\textbf{Hyperparameters}}          & \multicolumn{1}{l|}{\textbf{Value/ Category}}              & \multicolumn{1}{l|}{\textbf{Comments}}                                                                  \\ \hline
        \multicolumn{1}{|l|}{\textit{Learning rate}}            & \multicolumn{1}{l|}{0.01}                                  & \multicolumn{1}{l|}{Halved every 1000th batch}                                                          \\ \hline
        \multicolumn{1}{|l|}{\textit{Momentum}}                 & \multicolumn{1}{l|}{0.95}                                  & \multicolumn{1}{l|}{}                                                                                   \\ \hline
        \multicolumn{1}{|l|}{\textit{Loss function}}            & \multicolumn{1}{l|}{Weighted Cross-entropy}                & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}Background = 1, Other = 25,\\ Sandeel = 30\end{tabular}} \\ \hline
        \multicolumn{1}{|l|}{\textit{Optimizer}}                & \multicolumn{1}{l|}{Stochastic gradient descent}           & \multicolumn{1}{l|}{}                                                                                   \\ \hline
        \multicolumn{1}{|l|}{\textit{Batch size}}               & \multicolumn{1}{l|}{16}                                    & \multicolumn{1}{l|}{}                                                                                   \\ \hline
        \multicolumn{1}{|l|}{\textit{Crop size}}                & \multicolumn{1}{l|}{256x256}                               & \multicolumn{1}{l|}{Include all available channels}                                                     \\ \hline
        \end{tabular}
        \end{table}
        
        The only change to the model was the number of channels in the input layer called "input image" in figure \ref{unet_fig}. This 
        
    
    \subsection{Greedy-search frequencies - Decreasing amount}
        This experiment will look into the performance of a model that both trains and predicts on data containing sequentially fewer frequencies. Using the F1 score described in section \ref{f1_score} as a performance metric,  I trained 
        
    \subsubsection{Results}
        
    \subsection{Greedy-search frequencies - Increasing amount}
    
    \subsubsection{Results}
        

